{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66334af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43900d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2cdc2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('../data/processed_train.pkl')\n",
    "test_data = pd.read_pickle('../data/processed_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5e5a2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data.append(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b0a68bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eec2e29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0  when father is dysfunctional and is so selfish...   \n",
       "1   2    0.0  thanks for lyft credit cannot use cause they d...   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...  \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "79132960",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer1 = text.CountVectorizer(ngram_range=(1, 1), analyzer='word', binary=False,\n",
    "                                       max_df=0.9, max_features=1000, stop_words=stops)\n",
    "count_vectorizer2 = text.CountVectorizer(ngram_range=(1, 1), analyzer='word', binary=False,\n",
    "                                       max_df=0.9, max_features=1000, stop_words=stops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d5b95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_stemmed'] = data['tweet_stemmed'].apply(lambda x: ' '.join(x))\n",
    "data['tweet_lemmatized'] = data['tweet_lemmatized'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302cb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aa84ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_stemmed = count_vectorizer1.fit_transform(data['tweet_stemmed'], )\n",
    "bag_lemmantized = count_vectorizer2.fit_transform(data['tweet_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8f444399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49159 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abl  absolut  accept  account  act  action  actor  actual  ad  adapt  \\\n",
       "0        0        0       0        0    0       0      0       0   0      0   \n",
       "1        0        0       0        0    0       0      0       0   0      0   \n",
       "2        0        0       0        0    0       0      0       0   0      0   \n",
       "3        0        0       0        0    0       0      0       0   0      0   \n",
       "4        0        0       0        0    0       0      0       0   0      0   \n",
       "...    ...      ...     ...      ...  ...     ...    ...     ...  ..    ...   \n",
       "49154    0        0       0        0    0       0      0       0   0      0   \n",
       "49155    0        0       0        0    0       0      0       0   0      0   \n",
       "49156    0        0       0        0    0       0      0       0   0      0   \n",
       "49157    0        0       0        0    0       0      0       0   0      0   \n",
       "49158    0        0       0        0    0       0      0       0   0      0   \n",
       "\n",
       "       ...  year  yesterday  yet  yo  yoga  york  young  youtub  yr  yummi  \n",
       "0      ...     0          0    0   0     0     0      0       0   0      0  \n",
       "1      ...     0          0    0   0     0     0      0       0   0      0  \n",
       "2      ...     0          0    0   0     0     0      0       0   0      0  \n",
       "3      ...     0          0    0   0     0     0      0       0   0      0  \n",
       "4      ...     0          0    0   0     0     0      0       0   0      0  \n",
       "...    ...   ...        ...  ...  ..   ...   ...    ...     ...  ..    ...  \n",
       "49154  ...     0          0    0   0     0     0      0       0   0      0  \n",
       "49155  ...     0          0    0   0     0     0      0       0   0      0  \n",
       "49156  ...     0          0    0   0     0     0      0       0   0      0  \n",
       "49157  ...     0          0    0   0     0     0      0       0   0      0  \n",
       "49158  ...     0          0    0   0     0     0      0       0   0      0  \n",
       "\n",
       "[49159 rows x 1000 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names1 = count_vectorizer1.get_feature_names()\n",
    "pd.DataFrame(bag_stemmed.toarray(), columns = feature_names1)\n",
    "\n",
    "feature_names2 = count_vectorizer2.get_feature_names()\n",
    "pd.DataFrame(bag_lemmantized.toarray(), columns = feature_names2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2fa0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "54061c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = text.TfidfVectorizer(ngram_range=(1, 1), analyzer='word', binary=False,\n",
    "                                       max_df=0.9, max_features=1000, stop_words=stops)\n",
    "tfidf2 = text.TfidfVectorizer(ngram_range=(1, 1), analyzer='word', binary=False,\n",
    "                                       max_df=0.9, max_features=1000, stop_words=stops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "410ace91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49159 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       abl  absolut  accept  account  act  action  actor  actual   ad  adapt  \\\n",
       "0      0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0   \n",
       "1      0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0   \n",
       "2      0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0   \n",
       "3      0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0   \n",
       "4      0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0   \n",
       "...    ...      ...     ...      ...  ...     ...    ...     ...  ...    ...   \n",
       "49154  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0   \n",
       "49155  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0   \n",
       "49156  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0   \n",
       "49157  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0   \n",
       "49158  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0   \n",
       "\n",
       "       ...  year  yesterday  yet   yo  yoga  york  young  youtub   yr  yummi  \n",
       "0      ...   0.0        0.0  0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "1      ...   0.0        0.0  0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "2      ...   0.0        0.0  0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "3      ...   0.0        0.0  0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "4      ...   0.0        0.0  0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "...    ...   ...        ...  ...  ...   ...   ...    ...     ...  ...    ...  \n",
       "49154  ...   0.0        0.0  0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "49155  ...   0.0        0.0  0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "49156  ...   0.0        0.0  0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "49157  ...   0.0        0.0  0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "49158  ...   0.0        0.0  0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "\n",
       "[49159 rows x 1000 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_stemmed = tfidf1.fit_transform(data['tweet_stemmed'], )\n",
    "bag_lemmantized = tfidf2.fit_transform(data['tweet_lemmatized'])\n",
    "\n",
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names1 = tfidf1.get_feature_names()\n",
    "pd.DataFrame(bag_stemmed.toarray(), columns = feature_names1)\n",
    "\n",
    "feature_names2 = tfidf2.get_feature_names()\n",
    "pd.DataFrame(bag_lemmantized.toarray(), columns = feature_names2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "631300dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label\n",
       "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# создаем df\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "trainDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c183d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer1 = text.CountVectorizer(ngram_range=(1, 3), analyzer='word', binary=True,\n",
    "                                       max_df=0.9, max_features=1000, stop_words=stops)\n",
    "tfidf1 = text.TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False,\n",
    "                                       max_df=0.9, max_features=1500, stop_words=stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c77cf13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a19c0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9147860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1c5275d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "count_vectorizer1.fit(trainDF['text'])\n",
    "xtrain_count =  count_vectorizer1.transform(train_x)\n",
    "xvalid_count =  count_vectorizer1.transform(valid_x)\n",
    "\n",
    "tfidf1.fit(trainDF['text'])\n",
    "xtrain_count2 =  tfidf1.transform(train_x)\n",
    "xvalid_count2 =  tfidf1.transform(valid_x)\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(xtrain_count2, train_y)\n",
    "predictions = classifier.predict(xvalid_count2)\n",
    "acc = accuracy_score(valid_y, predictions)\n",
    "f1 = f1_score(valid_y, predictions)\n",
    "params = 'ngram_range=(1, 3), analyzer=word, binary=False, max_df=0.9, max_features=1500'\n",
    "results = results.append([['tfidf max_features=1500', acc, f1, params]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ca822b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.columns = ['name', 'accuracy', 'f1', 'params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "95e8267c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>0.8332</td>\n",
       "      <td>0.831379</td>\n",
       "      <td>ngram_range=(1, 1), analyzer=word, binary=Fals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.8324</td>\n",
       "      <td>0.829328</td>\n",
       "      <td>ngram_range=(1, 1), analyzer=word, binary=Fals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.8376</td>\n",
       "      <td>0.835761</td>\n",
       "      <td>ngram_range=(1, 1), analyzer=word, binary=Fals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf 3gram</td>\n",
       "      <td>0.8404</td>\n",
       "      <td>0.838134</td>\n",
       "      <td>ngram_range=(1, 3), analyzer=word, binary=True...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf 1 and 2gram</td>\n",
       "      <td>0.8396</td>\n",
       "      <td>0.834775</td>\n",
       "      <td>ngram_range=(1, 2), analyzer=word, binary=True...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf max_df=0.7</td>\n",
       "      <td>0.8384</td>\n",
       "      <td>0.830537</td>\n",
       "      <td>ngram_range=(1, 2), analyzer=word, binary=True...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf max_features=1500</td>\n",
       "      <td>0.8392</td>\n",
       "      <td>0.834568</td>\n",
       "      <td>ngram_range=(1, 3), analyzer=word, binary=Fals...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name  accuracy        f1  \\\n",
       "0                   count     0.8332  0.831379   \n",
       "0                   tfidf     0.8324  0.829328   \n",
       "0                   tfidf     0.8376  0.835761   \n",
       "0              tfidf 3gram    0.8404  0.838134   \n",
       "0        tfidf 1 and 2gram    0.8396  0.834775   \n",
       "0         tfidf max_df=0.7    0.8384  0.830537   \n",
       "0  tfidf max_features=1500    0.8392  0.834568   \n",
       "\n",
       "                                              params  \n",
       "0  ngram_range=(1, 1), analyzer=word, binary=Fals...  \n",
       "0  ngram_range=(1, 1), analyzer=word, binary=Fals...  \n",
       "0  ngram_range=(1, 1), analyzer=word, binary=Fals...  \n",
       "0  ngram_range=(1, 3), analyzer=word, binary=True...  \n",
       "0  ngram_range=(1, 2), analyzer=word, binary=True...  \n",
       "0  ngram_range=(1, 2), analyzer=word, binary=True...  \n",
       "0  ngram_range=(1, 3), analyzer=word, binary=Fals...  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "906e625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c0dbc5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8776bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sp.rand(1000, 1000, density=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006f001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7ad2b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TruncatedSVD(100)\n",
    "Xpca = clf.fit_transform(xtrain_count2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1197a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(svd_solver='full')\n",
    "X_train_pca = pca.fit_transform(Xpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "26c420df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpca_2 = clf.transform(xvalid_count2)\n",
    "X_test_pca = pca.transform(Xpca_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2712c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(X_train_pca, train_y)\n",
    "predictions = classifier.predict(X_test_pca)\n",
    "acc = accuracy_score(valid_y, predictions)\n",
    "f1 = f1_score(valid_y, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "552b9c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828\n",
      "0.8218724109362054\n"
     ]
    }
   ],
   "source": [
    "print(acc)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da823bf7",
   "metadata": {},
   "source": [
    "В результате исследования tfidf векторайзер сработал на данном примере лучше,\n",
    "триграмы с биграмами и униграммами дали лучший рещультат чем тотлько униграммы.\n",
    "Врядли это означает что мы нашли в текстах устойчивые выражения,\n",
    "скорее списал бы это на переобучение. Алгоритм уменьшения пространства pca, не дал\n",
    "прироста в метрике, но значительно повысил эффективность и убрал переобучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee5c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
